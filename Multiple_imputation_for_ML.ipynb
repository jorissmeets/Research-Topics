{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231ab17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras.metrics import categorical_accuracy\n",
    "import keras\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "random.set_seed(2)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "import keras.optimizers\n",
    "\n",
    "# for the scale data good hyperparameters are: nodes=500, batchsize=32, epochs=10\n",
    "# for the banknote data good parameters are: nodes=500, batchsize=20, n_epochs=3\n",
    "# for the user data good parameters are: nodes=500, batchsize=3, n_epochs=25\n",
    "dataset_name = 'user_data'\n",
    "n_MC = 21\n",
    "n_nodes = 500\n",
    "batchsize = 3\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dfe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(MD_percentage):\n",
    "    dataset_list = []\n",
    "    for i in ['a','b','c','d','e']:\n",
    "        file_path = 'C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/' + dataset_name + '/' + MC_ID + '/train' + MD_percentage + i + '.csv'\n",
    "        df_i = pd.read_csv(file_path, index_col=0)\n",
    "        data_i = df_i.to_numpy()\n",
    "        dataset_list.append(data_i)\n",
    "        \n",
    "    return dataset_list\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    file_path = 'C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/' + dataset_name + '/' + MC_ID +'/test.csv'\n",
    "    df_test = pd.read_csv(file_path,index_col=0)\n",
    "    test_data = df_test.to_numpy()\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "\n",
    "def build_features(dataset):    \n",
    "    count_dependent_vars = dataset.shape[1]-1\n",
    "    \n",
    "    X = dataset[:,0:count_dependent_vars]      \n",
    "    X = X.astype(float)\n",
    "    \n",
    "    y = dataset[:,count_dependent_vars]\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)    \n",
    "    y = to_categorical(encoded_y)\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_NN(X, y, input_dim, output_dim, nodes=n_nodes):    \n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()    \n",
    "    model.add(Dense(nodes, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "        \n",
    "    model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=categorical_accuracy)\n",
    "    \n",
    "    model.fit(X, y, epochs=n_epochs, batch_size=batchsize, verbose=False)\n",
    " \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_NN_custom(MD_percentage, custom_weights):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    with open('C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/{0}/{1}/model.json'.format(dataset_name, MC_ID), 'r') as json_file:\n",
    "        json_savedModel = json_file.read()\n",
    "\n",
    "    pooled_weights = pool_weight_matrices(MD_percentage)\n",
    "    \n",
    "    model_j = keras.models.model_from_json(json_savedModel)\n",
    "    model_j.set_weights(pooled_weights)\n",
    "    model_j.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=categorical_accuracy)\n",
    "    \n",
    "    return(model_j)\n",
    "   \n",
    "    \n",
    "def save_weights_to_npy(model, MD_percentage, model_tracker):\n",
    "    weights_filename = 'C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/' + dataset_name + '/' + MC_ID + '/model_weights' + MD_percentage + model_tracker + '.npy'\n",
    "    weights = model.get_weights()\n",
    "    np.save(weights_filename, weights)\n",
    "    \n",
    "    \n",
    "def save_model_to_json(model):\n",
    "    model_json = model.to_json()\n",
    "    json_filename = 'C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/' + dataset_name + '/' + MC_ID + '/model.json'\n",
    "    with open(json_filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)     \n",
    "\n",
    "        \n",
    "def evaluate_nn_model(model):\n",
    "    dataset = test_data\n",
    "    X, y = build_features(dataset)\n",
    "    \n",
    "    y_cat = y.round()\n",
    "    y_cat = np.argmax(y_cat, axis=1)\n",
    "    \n",
    "    y_hat = model.predict(X)\n",
    "    y_hat = y_hat.round()\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "    \n",
    "    res = pd.DataFrame({'y':y_cat,'y_hat':y_hat,'correct?':(y_cat==y_hat)})    \n",
    "    accuracy = len(res[res['correct?']==True])/len(res)*100\n",
    "        \n",
    "    return round(accuracy,3), res\n",
    "\n",
    "        \n",
    "# METHOD 1\n",
    "def save_model_and_weights(MD_percentage):\n",
    "    dataset_list = load_train_data(MD_percentage)\n",
    "\n",
    "    count = 0\n",
    "    models = ['a','b','c','d','e']\n",
    "    for dataset in dataset_list:\n",
    "        model_tracker = models[count]\n",
    "        X, y = build_features(dataset)\n",
    "        model = build_NN(X, y, input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "        \n",
    "        save_model_to_json(model)\n",
    "        save_weights_to_npy(model, MD_percentage, model_tracker)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "# METHOD 1\n",
    "def pool_weight_matrices(MD_percentage):\n",
    "    weight_matrix_collections = []\n",
    "    models = ['a','b','c','d','e']\n",
    "\n",
    "    for i in range(0,5):\n",
    "        model_tracker = models[i]\n",
    "        weights_filename = 'C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/' + dataset_name+ '/' + MC_ID + '/model_weights' + MD_percentage + model_tracker + '.npy'\n",
    "        file = np.load(weights_filename, allow_pickle=True)\n",
    "        weight_matrix_collections.append(file)\n",
    "\n",
    "    average_model_weights = []\n",
    "    for array in range(0,len(weight_matrix_collections[0])):\n",
    "        array_sum = weight_matrix_collections[0][array] + weight_matrix_collections[1][array] + weight_matrix_collections[2][array] + weight_matrix_collections[3][array] + weight_matrix_collections[4][array]\n",
    "        array_average = array_sum/5\n",
    "        average_model_weights.append(array_average)\n",
    "    return average_model_weights\n",
    "\n",
    "\n",
    "# METHOD 2\n",
    "def pool_datasets(MD_percentage):\n",
    "    dataset_list = load_train_data(MD_percentage)\n",
    "\n",
    "    for i in range(0,5):\n",
    "        dataset_list[i] = pd.DataFrame(dataset_list[i])\n",
    "          \n",
    "        \n",
    "    y = dataset_list[0].iloc[:,-1]    \n",
    "    df_summed_x = dataset_list[0].iloc[:,:-1]\n",
    "        \n",
    "    for i in range(1,5):\n",
    "        df_summed_x = df_summed_x + dataset_list[i].iloc[:,:-1]\n",
    "        \n",
    "    average_df = df_summed_x/5\n",
    "    \n",
    "    y_index = df_summed_x.shape[1]\n",
    "    \n",
    "    average_df[y_index] = y\n",
    "    \n",
    "    averaged_data = average_df.to_numpy()\n",
    "    \n",
    "    return averaged_data\n",
    "\n",
    "\n",
    "# METHOD 3\n",
    "def get_bagging_predictions(dataset):\n",
    "    X, y = build_features(dataset)\n",
    "        \n",
    "    model = build_NN(X, y, input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "    \n",
    "    evaluation = evaluate_nn_model(model)[1]\n",
    "    \n",
    "    y_cat = evaluation['y']\n",
    "    y_hat = evaluation['y_hat']\n",
    "    \n",
    "    return y_hat, y_cat\n",
    "\n",
    "\n",
    "def run_method1():\n",
    "    accuracy_list = []\n",
    "    for MD_percentage in ['10','20','50']:\n",
    "        save_model_and_weights(MD_percentage)\n",
    "        custom_weights = pool_weight_matrices(MD_percentage)\n",
    "                \n",
    "        model_j = build_NN_custom(MD_percentage, custom_weights)\n",
    "                \n",
    "        accuracy_on_test = evaluate_nn_model(model_j)[0]\n",
    "        accuracy_list.append(accuracy_on_test)\n",
    "        \n",
    "    return accuracy_list\n",
    "\n",
    "\n",
    "def run_method2():\n",
    "    accuracy_list = []\n",
    "    for MD_percentage in ['10','20','50']:\n",
    "        averaged_data = pool_datasets(MD_percentage)\n",
    "        \n",
    "        X, y = build_features(averaged_data)\n",
    "                    \n",
    "        model = build_NN(X,y,input_dim=X.shape[1],output_dim=y.shape[1])\n",
    "        \n",
    "        accuracy_on_test = evaluate_nn_model(model)[0]\n",
    "        accuracy_list.append(accuracy_on_test)\n",
    "        \n",
    "    return accuracy_list\n",
    "\n",
    "\n",
    "def run_method3():\n",
    "    accuracy_list = []\n",
    "    for MD_percentage in ['10','20','50']:\n",
    "        dataset_list = load_train_data(MD_percentage)\n",
    "        pred_df = pd.DataFrame()\n",
    "        for i in range(0,5):\n",
    "            pred_df[i], y = get_bagging_predictions(dataset_list[i])\n",
    "        \n",
    "        final_y_hat = pred_df.mode(axis=1)[0]\n",
    "                \n",
    "        compare_df = pd.DataFrame({'y_hat':final_y_hat,'y':y})\n",
    "        \n",
    "        res = pd.DataFrame({'y':y,'y_hat':final_y_hat,'correct?':(y==final_y_hat)})    \n",
    "        accuracy = len(res[res['correct?']==True])/len(res)*100\n",
    "    \n",
    "        accuracy_list.append(accuracy)\n",
    "    return accuracy_list\n",
    "\n",
    "\n",
    "def run_baseline():      \n",
    "    file_path = 'C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/' + dataset_name + '/' + MC_ID + '/train.csv'\n",
    "    df_i = pd.read_csv(file_path, index_col=0)\n",
    "    np_data = df_i.to_numpy()\n",
    "\n",
    "    X, y = build_features(np_data)\n",
    "    \n",
    "    model = build_NN(X, y, input_dim=X.shape[1],output_dim=y.shape[1])\n",
    "    \n",
    "    accuracy_on_test = evaluate_nn_model(model)[0]\n",
    "    \n",
    "    return accuracy_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f677051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-362719128267>:4: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  baseline_series = pd.Series()\n",
      "C:\\Users\\20175878\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 method 1 is done\n",
      "iteration 1 method 2 is done\n",
      "iteration 1 method 3 is done\n",
      "iteration 1 baseline is done\n",
      "running iteration 1 took 0:02:31.904389\n",
      "iteration 2 method 1 is done\n",
      "iteration 2 method 2 is done\n",
      "iteration 2 method 3 is done\n",
      "iteration 2 baseline is done\n",
      "running iteration 2 took 0:02:33.201261\n",
      "iteration 3 method 1 is done\n",
      "iteration 3 method 2 is done\n",
      "iteration 3 method 3 is done\n",
      "iteration 3 baseline is done\n",
      "running iteration 3 took 0:02:33.700254\n",
      "iteration 4 method 1 is done\n",
      "iteration 4 method 2 is done\n",
      "iteration 4 method 3 is done\n",
      "iteration 4 baseline is done\n",
      "running iteration 4 took 0:02:35.034339\n",
      "iteration 5 method 1 is done\n",
      "iteration 5 method 2 is done\n",
      "iteration 5 method 3 is done\n",
      "iteration 5 baseline is done\n",
      "running iteration 5 took 0:02:33.168273\n",
      "iteration 6 method 1 is done\n",
      "iteration 6 method 2 is done\n",
      "iteration 6 method 3 is done\n",
      "iteration 6 baseline is done\n",
      "running iteration 6 took 0:02:33.754812\n",
      "iteration 7 method 1 is done\n",
      "iteration 7 method 2 is done\n",
      "iteration 7 method 3 is done\n",
      "iteration 7 baseline is done\n",
      "running iteration 7 took 0:02:32.369856\n",
      "iteration 8 method 1 is done\n",
      "iteration 8 method 2 is done\n",
      "iteration 8 method 3 is done\n",
      "iteration 8 baseline is done\n",
      "running iteration 8 took 0:02:34.323560\n",
      "iteration 9 method 1 is done\n",
      "iteration 9 method 2 is done\n",
      "iteration 9 method 3 is done\n",
      "iteration 9 baseline is done\n",
      "running iteration 9 took 0:02:33.006461\n",
      "iteration 10 method 1 is done\n",
      "iteration 10 method 2 is done\n",
      "iteration 10 method 3 is done\n",
      "iteration 10 baseline is done\n",
      "running iteration 10 took 0:02:33.594679\n",
      "iteration 11 method 1 is done\n",
      "iteration 11 method 2 is done\n",
      "iteration 11 method 3 is done\n",
      "iteration 11 baseline is done\n",
      "running iteration 11 took 0:02:36.277842\n",
      "iteration 12 method 1 is done\n",
      "iteration 12 method 2 is done\n",
      "iteration 12 method 3 is done\n",
      "iteration 12 baseline is done\n",
      "running iteration 12 took 0:02:57.068745\n",
      "iteration 13 method 1 is done\n",
      "iteration 13 method 2 is done\n",
      "iteration 13 method 3 is done\n",
      "iteration 13 baseline is done\n",
      "running iteration 13 took 0:02:58.357725\n",
      "iteration 14 method 1 is done\n",
      "iteration 14 method 2 is done\n",
      "iteration 14 method 3 is done\n",
      "iteration 14 baseline is done\n",
      "running iteration 14 took 0:02:55.191500\n",
      "iteration 15 method 1 is done\n",
      "iteration 15 method 2 is done\n",
      "iteration 15 method 3 is done\n",
      "iteration 15 baseline is done\n",
      "running iteration 15 took 0:02:57.276624\n",
      "iteration 16 method 1 is done\n",
      "iteration 16 method 2 is done\n",
      "iteration 16 method 3 is done\n",
      "iteration 16 baseline is done\n",
      "running iteration 16 took 0:03:00.870452\n",
      "iteration 17 method 1 is done\n",
      "iteration 17 method 2 is done\n",
      "iteration 17 method 3 is done\n",
      "iteration 17 baseline is done\n",
      "running iteration 17 took 0:02:05.659562\n",
      "iteration 18 method 1 is done\n",
      "iteration 18 method 2 is done\n",
      "iteration 18 method 3 is done\n",
      "iteration 18 baseline is done\n",
      "running iteration 18 took 0:02:44.370485\n",
      "iteration 19 method 1 is done\n",
      "iteration 19 method 2 is done\n",
      "iteration 19 method 3 is done\n",
      "iteration 19 baseline is done\n",
      "running iteration 19 took 0:04:09.259728\n",
      "iteration 20 method 1 is done\n",
      "iteration 20 method 2 is done\n",
      "iteration 20 method 3 is done\n",
      "iteration 20 baseline is done\n",
      "running iteration 20 took 0:01:25.210389\n",
      "running the whole MC simulation took 0:53:23.601924\n"
     ]
    }
   ],
   "source": [
    "method1_df = pd.DataFrame(columns=['10','20','50'])\n",
    "method2_df = pd.DataFrame(columns=['10','20','50'])\n",
    "method3_df = pd.DataFrame(columns=['10','20','50'])\n",
    "baseline_series = pd.Series()\n",
    "\n",
    "start_time = datetime.now()\n",
    "for i in range(1,n_MC):\n",
    "    start_time_i = datetime.now()\n",
    "    MC_ID = str(i)\n",
    "    test_data = load_test_data()\n",
    "    \n",
    "    method1_res = pd.Series(run_method1(), index = method1_df.columns)\n",
    "    method1_df = method1_df.append(method1_res, ignore_index=True)\n",
    "    print('iteration {} method 1 is done'.format(i))\n",
    "    \n",
    "    method2_res = pd.Series(run_method2(), index = method2_df.columns)\n",
    "    method2_df = method2_df.append(method2_res, ignore_index=True)\n",
    "    print('iteration {} method 2 is done'.format(i))\n",
    "    \n",
    "    method3_res = pd.Series(run_method3(), index = method3_df.columns)\n",
    "    method3_df = method3_df.append(method3_res, ignore_index=True)\n",
    "    print('iteration {} method 3 is done'.format(i))\n",
    "    \n",
    "    baseline_series = baseline_series.append(pd.Series(run_baseline()))\n",
    "    print('iteration {} baseline is done'.format(i))\n",
    "    \n",
    "    print('running iteration {0} took {1}'.format(i, datetime.now() - start_time_i))\n",
    "\n",
    "print('running the whole MC simulation took {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea5fb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "method1_df.to_csv('C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/MC_results/{0}__nodes{1}_batch{2}_epoch{3}_method1_results.csv'.format(dataset_name, n_nodes, batchsize, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d48964",
   "metadata": {},
   "outputs": [],
   "source": [
    "method2_df.to_csv('C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/MC_results/{0}__nodes{1}_batch{2}_epoch{3}_method2_results.csv'.format(dataset_name, n_nodes, batchsize, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04ea9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "method3_df.to_csv('C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/MC_results/{0}__nodes{1}_batch{2}_epoch{3}_method3_results.csv'.format(dataset_name, n_nodes, batchsize, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38c3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_series.to_csv('C:/Users/20175878/Documents/DSAI/Y1Q1_RTDM/research_paper/code/MC_results/{0}__nodes{1}_batch{2}_epoch{3}_baseline_results.csv'.format(dataset_name, n_nodes, batchsize, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b1b288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.215</td>\n",
       "      <td>94.215</td>\n",
       "      <td>91.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.909</td>\n",
       "      <td>90.909</td>\n",
       "      <td>86.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.256</td>\n",
       "      <td>89.256</td>\n",
       "      <td>84.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.124</td>\n",
       "      <td>87.603</td>\n",
       "      <td>80.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.736</td>\n",
       "      <td>87.603</td>\n",
       "      <td>75.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.909</td>\n",
       "      <td>90.909</td>\n",
       "      <td>77.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90.083</td>\n",
       "      <td>89.256</td>\n",
       "      <td>82.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>89.256</td>\n",
       "      <td>89.256</td>\n",
       "      <td>85.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94.215</td>\n",
       "      <td>93.388</td>\n",
       "      <td>91.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.083</td>\n",
       "      <td>89.256</td>\n",
       "      <td>76.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90.909</td>\n",
       "      <td>91.736</td>\n",
       "      <td>84.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94.215</td>\n",
       "      <td>94.215</td>\n",
       "      <td>85.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>91.736</td>\n",
       "      <td>88.430</td>\n",
       "      <td>84.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91.736</td>\n",
       "      <td>85.124</td>\n",
       "      <td>80.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>91.736</td>\n",
       "      <td>90.083</td>\n",
       "      <td>89.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>89.256</td>\n",
       "      <td>88.430</td>\n",
       "      <td>85.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>92.562</td>\n",
       "      <td>93.388</td>\n",
       "      <td>87.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>92.562</td>\n",
       "      <td>92.562</td>\n",
       "      <td>87.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>92.562</td>\n",
       "      <td>90.909</td>\n",
       "      <td>83.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>89.256</td>\n",
       "      <td>90.909</td>\n",
       "      <td>84.298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        10      20      50\n",
       "0   94.215  94.215  91.736\n",
       "1   90.909  90.909  86.777\n",
       "2   89.256  89.256  84.298\n",
       "3   85.124  87.603  80.165\n",
       "4   91.736  87.603  75.207\n",
       "5   90.909  90.909  77.686\n",
       "6   90.083  89.256  82.645\n",
       "7   89.256  89.256  85.124\n",
       "8   94.215  93.388  91.736\n",
       "9   90.083  89.256  76.860\n",
       "10  90.909  91.736  84.298\n",
       "11  94.215  94.215  85.950\n",
       "12  91.736  88.430  84.298\n",
       "13  91.736  85.124  80.165\n",
       "14  91.736  90.083  89.256\n",
       "15  89.256  88.430  85.124\n",
       "16  92.562  93.388  87.603\n",
       "17  92.562  92.562  87.603\n",
       "18  92.562  90.909  83.471\n",
       "19  89.256  90.909  84.298"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21d33b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.909\n",
      "90.909\n",
      "91.736\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(run_baseline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c990798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
